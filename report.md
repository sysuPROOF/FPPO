# Federated Proximal policy Optimization Algorithm

## Abstract

> 这是一段的，大约两百字，我来写。

## Introduction

> 这个分很多段的，大约1500字，你们分着写。
>
> 第一段：介绍联邦学习 杨强他有一篇联邦学习的论文 https://www.fedai.org.cn/
>
> 第二段：介绍强化学习 spingup 博客抄一下https://spinningup.openai.com/en/latest/
>
> 第三段：介绍联邦强化学习现有的工作，及其他们的不足。 把老师的论文内容翻译一下
>
> 第四段：针对以前联邦强化学习的不足，我们采取了什么的方法，进行了哪些工作。 以前的联邦强化学习都是离散动作的，用DQN，我们连续动作，用PG
>
> 第五段：对我们工作的总结，有什么贡献。 怎么厉害怎么说，就解决世界和平的问题。

## Related Work

> 分三段，大概写500字
>
> 第一段：联邦学习 *5篇左右*
>
> 第二段：分布式强化学习
>
> 第三段：多智能体强化学习

## Problem Definition

https://spinningup.openai.com/en/latest/

> 强化学习的优化目标
>
> 策略梯度的一些基本概念

## Our FPPO Approach

> PPO 的公式
>
> 联邦学习的公式
>
> FPPO的算法

## Experiments

> 三个实验：
>
> 1. gym做一个算法验证收敛性的实验

## Conclusion

> 一段大概200字

## References

> 参考文献

Leveraging Procedural Generation to BenchmarkReinforcement Learning
