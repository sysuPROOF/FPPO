# Federated Proximal policy Optimization Algorithm

## Abstract

> 这是一段的，大约两百字，我来写。

## Introduction

> 联邦学习
>
> 在智能算法实际应用的过程中，可能会面临以下两个问题：一是在大多数行业中，数据是以孤岛的形式存在的，智能算法实现所需的数据源之间存在着难以打破的壁垒；二是数据安全和隐私越来越受到重视，意味着智能算法所需要的数据更加难以转移和获取。为解决智能算法所需要的数据量与实际能够获取的数据量不匹配的问题，QIANG$^{[1]}$等学者提出了一个满足隐私保护和数据安全的可行的解决方案——联邦学习。
>
> 联邦学习可以理解为一种保护数据隐私的分散协作机器学习技术。例如在我们的实验中，有玩家A和玩家B，它们用相似的算法模型产生了不同的经验数据，而利用联邦学习可以做到让各玩家的自有数据不出本地，而后联邦系统可以通过加密机制下的参数交换方式，即在不违反数据隐私法规情况下，建立一个虚拟的共有模型。这个虚拟模型就好像大家把数据聚合在一起建立的最优模型一样。但是在建立虚拟模型的时候，数据本身不移动，也不泄露隐私和影响数据合规。这样，建好的模型在各自区域仅为本地的目标服务。在这样一个联邦机制下，各个玩家的身份地位相同，而联邦系统帮助大家建立了更优的策略解决方案。
>
> 强化学习
>
> 强化学习（RL）是机器学习中的一个领域，强调如何基于环境而行动，以取得最大化的预期利益。RL的主要特征是智能体（Agent）和环境（Environment）。 在机器学习问题中，环境通常被规范为马尔可夫决策过程（MDP），简单说就是一个智能体采取行动（Action）从而改变自己的状态（State）获得奖励（Reward）与环境发生交互的循环过程，如图1。 智能体的目标是最大限度地提高其累积报酬，称为回报（Return）。强化学习方法是智能体能通过学习行为以实现其目标的方式。
>
>![1](https://github.com/sysuPROOF/FPPO/tree/master/images "1")
>
>强化学习实际上是一类算法的统称，其中最重要的分支点之一是Agent是否可以访问（或学习）环境模型的问题，通过环境模型可以定义预测状态转换和奖励的函数。根据这点RL算法可以分为Model-Free以及Model-Based两种类型。尽管Model-Free方法没有使用模型可能带来的提高样本效率的潜力，但它们往往更易于实现和调整，所以比Model-Based方法更受欢迎。Model-Free RL中比较有代表性的是策略最优（Policy Optimization）以及Q-learning的方法。我们实验中主要使用的是策略最优方法中基于策略梯度（Policy Gradient）的强化学习算法。
>
> 联邦强化学习
>
>联邦强化学习实质是隐私保护的分布式强化学习，它的提出是基于在强化学习中考虑联合设置（即多智能体协作）的场景。如果是共享环境中的一组智能体，即多智能体强化学习（MARL），就会带来这样一个问题：由于共享环境中其他智能体的交互，从智能体的角度来看多，多智能体域是非平稳的。同时由于MARL假设智能体的观察结果是共享的，所有代理都能获得奖励，动作空间就会随智能体数量成指数型增长，导致学习变得非常困难。与MARL不同的是，联邦强化学习假设智能体不共享其部分观察结果，意味着一些智能体无法获得奖励。联邦强化学习的作用在于，既可以保护每个智能体的数据隐私，又可以达到协作的目的，减少学习花费的时间。我们优秀的卓老师$^{[2]}$等学者提出了一个联邦深度强化学习（FedRL）的框架，是通过在智能体之间共享有限的信息（例如Q-network的输出）来为每个智能体学习一个私有的Q-network策略。信息发送时是加密的，收到时是已经解密的。FedRL假设一些智能体具有与状态和行为相对应的奖励，而其他智能体只观察状态而没有奖励，同时声明，所有智能体都能从加入联邦中获利来帮助构建自身的决策策略。
>
> 联邦强化学习的改进
>
> 在以往的联邦强化学习的研究中，解决的都是输出离散动作的问题（比如Grid-World域、Atari游戏Pong等，每个智能体最终达到目标的策略是一系列离散动作的组合）。这些研究几乎都是在联邦环境中使用Deep Q Network（DQN）强化学习算法。DQN在对高维状态输入、低维动作输出的问题应用上有着很好的效果。但由于DQN是通过计算动作得分（通过价值函数来选取动作）来决策的，即一种间接优化智能体性能的方式，是无法解决连续动作策略问题的（例如我们实验中的Mountaincar continuous游戏）。这种情况下就需要一种直接针对策略的优化方法，所以我们在联邦强化学习中采取了一种新的策略梯度算法——Proximal Policy Optimization（PPO），既可以解决离散动作策略问题又可以解决连续动作策略问题。为此我们做了两个实验分别在Cartpole游戏（离散动作）以及Mountaincar continuous游戏（连续动作）进行算法的应用测试。
>
>我们同样使用了基于C/S架构的联邦强化学习，即多个客户端节点保存本地训练数据以训练客户端模型，根据通信策略，客户端将其模型信息共享给一个中心服务器节点，并在该服务器节点上构建全局模型。在每个客户端节点中，都有一个智能体（PPO模型）学习游戏通关的策略。在离散动作的场景下，客户端使用Actor-Critic共享网络权重；在连续动作的场景下，客户端分别用各自的网络权重。离散与连续动作在模型上的区别在于网络的输出结构不同。服务器每次从所有客户端接收到客户模型信息后，用迭代网络加权平均更新全局模型。客户端在收到新的全局模型后，会更新自身的网络权重，以期获得更多的奖励。在经过一轮一轮的迭代更新后最终能够收敛到最优的策略。
>
>总结
>
> 我们的工作主要是实现了一个基于策略梯度的联邦强化学习算法（FPPO），它主要有以下三个优势：1）可以在保护用户数据的隐私的情况下完成分布式学习； 2）提高模型的收敛速度；3）既可以解决离散动作策略问题又可以解决连续动作策略问题。我们通过在gym上选取离散动作Cartpole和连续动作Mountaincar continuous游戏实验验证了算法的收敛性。该算法可以扩展到一些数据隐私和模型收敛速度都很重要的场景，例如，军事领域中的大规模联合作战的场景，可以使用联邦强化学习来应对作战单位之间的通信隐私和数据隐私问题，应对突发情况时的反应速度，拦截导弹的反应时间等等问题，维护世界和平。
>
>在报告的剩余部分，我们首先回顾了与联邦强化学习相关的工作；然后提出我们FPPO算法的问题定义和公式并详细介绍FPPO的框架；最后我们在gym中做验证FPPO收敛性的实验，并对该算法进行评估。

## Related Work

> 分三段，大概写500字
>
> 第一段：联邦学习 *5篇左右*
>
> 第二段：分布式强化学习
>
> 第三段：多智能体强化学习

## Problem Definition

> <https://spinningup.openai.com/en/latest>
>
> 强化学习的优化目标
>
> 策略梯度的一些基本概念

## Our FPPO Approach

> PPO 的公式
>
> 联邦学习的公式
>
> FPPO的算法

## Experiments

> 三个实验：
>
> 1. gym做一个算法验证收敛性的实验

## Conclusion

> 一段大概200字

## References

> 参考文献

Leveraging Procedural Generation to BenchmarkReinforcement Learning
